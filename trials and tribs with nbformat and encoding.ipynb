{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import Markdown\n",
    "from bs4 import UnicodeDammit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jupyter noetbook created with nbformat is unreadable beacuse invalid start byte.\n",
    "\n",
    "I'm trying to scrape the offical python docs into a Jupyter notebook format. The code below is retrieves the documentation for the difflib module parses it and creates two notebook files one with a single code cell the other has a single markdown cell. The notebook that contains the code cell opens flawlessly. However the notebook that contains the markdown cell will not open because of an 'invailid start byte' the full error message can be found below the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib import request\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "import nbformat as nbf\n",
    "\n",
    "# Get the contents of a python docs page.\n",
    "url = 'https://docs.python.org/3.6/library/difflib.html'\n",
    "req = request.urlopen(url)\n",
    "info = req.read()\n",
    "soup = BeautifulSoup(info, 'html.parser')\n",
    "\n",
    "# Isolate the portion of the page that we want.\n",
    "soup = soup.body\n",
    "sections = soup.find_all('div', class_='section')\n",
    "main_section = sections[0]\n",
    "str_soup = main_section.decode()\n",
    "\n",
    "# Get all of the Python code divs\n",
    "py = soup.find_all(class_=re.compile('python'))\n",
    "\n",
    "def clean_up(dirty_code):\n",
    "    \"\"\"Clean up sphinx formatted code.\"\"\"\n",
    "    # Use the re module to find working code.\n",
    "    if '>>>' in dirty_code.text:\n",
    "        result = re.findall('^>{3}(.+)|^\\.{3}(.+)', dirty_code.text, flags=re.M)\n",
    "        result = '\\n'.join([[k[1:] for k in i if len(k)>0][0] for i in result if len(i)>0])\n",
    "        return result\n",
    "    else:\n",
    "        return dirty_code.text\n",
    "    \n",
    "# Split up the text on every code div\n",
    "first_part = str_soup.split(str(py[0]))\n",
    "start = str_soup.split(str(py[0]))\n",
    "chunks = [start[0]]\n",
    "remainder = start[1]\n",
    "\n",
    "for i in range(1,len(py)):\n",
    "    parts = remainder.split(str(py[i]))\n",
    "    chunks.append(parts[0])\n",
    "    if len(parts)>1:\n",
    "        remainder = parts[1]\n",
    "\n",
    "# Create the notebook object.\n",
    "nb = nbf.v4.new_notebook()\n",
    "\n",
    "# TESTING PYTHON CODE CELL\n",
    "# Adding cell that imports all of the difflib modules.\n",
    "code_cell = nbf.v4.new_code_cell('from difflib import *')\n",
    "nb['cells'].append(code_cell)\n",
    "\n",
    "# Adding code scraped from the difflib docs.\n",
    "code_cell = nbf.v4.new_code_cell(clean_up(py[1]))\n",
    "nb['cells'].append(code_cell)\n",
    "\n",
    "# Write the single cell to a notebook file.\n",
    "fname = 'scraped_sphinx_code_test.ipynb'\n",
    "# This file opens with no problems the code is excutable \n",
    "with open(fname, 'w') as f:\n",
    "    nbf.write(nb, f)\n",
    "\n",
    "# Create another notebook object.\n",
    "nb = nbf.v4.new_notebook()\n",
    "\n",
    "# Test by formatting a chunk of HTML as a markdown cell.\n",
    "markdown_cell = nbf.v4.new_markdown_cell(chunks[1])\n",
    "nb['cells'].append(markdown_cell)\n",
    "\n",
    "# Write the single cell to a notebook file.\n",
    "# This file fails to open -> Notebook is Unreadable\n",
    "fname = 'scraped_sphinx_markdown_test.ipynb'\n",
    "with open(fname, 'w') as f:\n",
    "    nbf.write(nb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR ON OPENING\n",
    "\n",
    "Unreadable Notebook: C:\\scraped_sphinx_markdown_test.ipynb UnicodeDecodeError('utf-8', b'{\\r\\n \"cells\": [\\r\\n {\\r\\n \"cell_type\": \"markdown\",\\r\\n \"metadata\": {},\\r\\n \"source\": [\\r\\n \"\\\\n\",\\r\\n \"<p>See <a class=\\\\\"reference internal\\\\\" href=\\\\\"#difflib-interface\\\\\"><span class=\\\\\"std std-ref\\\\\">A command-line interface to difflib</span></a> for a more detailed example.</p>\\\\n\",\\r\\n \"</dd></dl>\\\\n\",\\r\\n \"<dl class=\\\\\"function\\\\\">\\\\n\",\\r\\n \"<dt id=\\\\\"difflib.get_close_matches\\\\\">\\\\n\",\\r\\n \"<code class=\\\\\"descclassname\\\\\">difflib.</code><code class=\\\\\"descname\\\\\">get_close_matches</code><span class=\\\\\"sig-paren\\\\\">(</span><em>word</em>, <em>possibilities</em>, <em>n=3</em>, <em>cutoff=0.6</em><span class=\\\\\"sig-paren\\\\\">)</span><a class=\\\\\"headerlink\\\\\" href=\\\\\"#difflib.get_close_matches\\\\\" title=\\\\\"Permalink to this definition\\\\\">\\xb6</a></dt>\\\\n\",\\r\\n \"<dd><p>Return a list of the best \\x93good enough\\x94 matches. <em>word</em> is a sequence for which\\\\n\",\\r\\n \"close matches are desired (typically a string), and <em>possibilities</em> is a list of\\\\n\",\\r\\n \"sequences against which to match <em>word</em> (typically a list of strings).</p>\\\\n\",\\r\\n \"<p>Optional argument <em>n</em> (default <code class=\\\\\"docutils literal\\\\\"><span class=\\\\\"pre\\\\\">3</span></code>) is the maximum number of close matches to\\\\n\",\\r\\n \"return; <em>n</em> must be greater than <code class=\\\\\"docutils literal\\\\\"><span class=\\\\\"pre\\\\\">0</span></code>.</p>\\\\n\",\\r\\n \"<p>Optional argument <em>cutoff</em> (default <code class=\\\\\"docutils literal\\\\\"><span class=\\\\\"pre\\\\\">0.6</span></code>) is a float in the range [0, 1].\\\\n\",\\r\\n \"Possibilities that don\\x92t score at least that similar to <em>word</em> are ignored.</p>\\\\n\",\\r\\n \"<p>The best (no more than <em>n</em>) matches among the possibilities are returned in a\\\\n\",\\r\\n \"list, sorted by similarity score, most similar first.</p>\\\\n\"\\r\\n ]\\r\\n }\\r\\n ],\\r\\n \"metadata\": {},\\r\\n \"nbformat\": 4,\\r\\n \"nbformat_minor\": 2\\r\\n}\\r\\n', 723, 724, 'invalid start byte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cells = []\n",
    "# for i in range(len(py)):\n",
    "#     markdown = nbf.v4.new_markdown_cell(chunks[i])\n",
    "#     cells.append(markdown)\n",
    "#     cell = nbf.v4.new_code_cell(clean_up(py[i]))\n",
    "#     cells.append(cell)\n",
    "\n",
    "# chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib import request\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "import nbformat as nbf\n",
    "\n",
    "# Get the contents of a python docs page.\n",
    "url = 'https://docs.python.org/3.6/library/difflib.html'\n",
    "req = request.urlopen(url)\n",
    "info = req.read().decode('UTF8')\n",
    "# info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(info, 'html.parser')\n",
    "\n",
    "# Isolate the portion of the page that we want.\n",
    "soup = soup.body\n",
    "sections = soup.find_all('div', class_='section')\n",
    "main_section = sections[0]\n",
    "# str_soup = main_section.decode()\n",
    "# str_soup = main_section.encode('ascii').decode()\n",
    "# Markdown(str_soup.decode())\n",
    "str_soup = main_section.prettify()\n",
    "# type(main_section)\n",
    "# str_soup = str_soup.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the notebook object.\n",
    "nb = nbf.v4.new_notebook()\n",
    "\n",
    "# Test by formatting a chunk of HTML as a markdown cell.\n",
    "markdown_cell = nbf.v4.new_markdown_cell(str_soup)\n",
    "\n",
    "nb['cells'].append(markdown_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbf.validator.isvalid(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the single cell to a notebook file.\n",
    "# This file fails to open -> Notebook is Unreadable\n",
    "fname = 'scraped_sphinx_markdown_test.ipynb'\n",
    "with open(fname, 'w') as f:\n",
    "    f.write(nbf.v4.writes(nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all of the Python code divs\n",
    "py = soup.find_all(class_=re.compile('python'))\n",
    "\n",
    "def clean_up(dirty_code):\n",
    "    \"\"\"Clean up sphinx formatted code.\"\"\"\n",
    "    # Use the re module to find working code.\n",
    "    if '>>>' in dirty_code.text:\n",
    "        result = re.findall('^>{3}(.+)|^\\.{3}(.+)', dirty_code.text, flags=re.M)\n",
    "        result = '\\n'.join([[k[1:] for k in i if len(k)>0][0] for i in result if len(i)>0])\n",
    "        return result\n",
    "    else:\n",
    "        return dirty_code.text\n",
    "    \n",
    "# Split up the text on every code div\n",
    "first_part = str_soup.split(str(py[0]))\n",
    "start = str_soup.split(str(py[0]))\n",
    "chunks = [start[0]]\n",
    "remainder = start[1]\n",
    "\n",
    "for i in range(1,len(py)):\n",
    "    parts = remainder.split(str(py[i]))\n",
    "    chunks.append(parts[0])\n",
    "    if len(parts)>1:\n",
    "        remainder = parts[1]\n",
    "\n",
    "# Create the notebook object.\n",
    "nb = nbf.v4.new_notebook()\n",
    "\n",
    "# TESTING PYTHON CODE CELL\n",
    "# Adding cell that imports all of the difflib modules.\n",
    "code_cell = nbf.v4.new_code_cell('from difflib import *')\n",
    "nb['cells'].append(code_cell)\n",
    "\n",
    "# Adding code scraped from the difflib docs.\n",
    "code_cell = nbf.v4.new_code_cell(clean_up(py[1]))\n",
    "nb['cells'].append(code_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"from difflib import *\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"get_close_matches(\\'appel\\', [\\'ape\\', \\'apple\\', \\'peach\\', \\'puppy\\'])\\\\n\",\\n    \"import keyword\\\\n\",\\n    \"get_close_matches(\\'wheel\\', keyword.kwlist)\\\\n\",\\n    \"get_close_matches(\\'pineapple\\', keyword.kwlist)\\\\n\",\\n    \"get_close_matches(\\'accept\\', keyword.kwlist)\"\\n   ]\\n  }\\n ],\\n \"metadata\": {},\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the single cell to a notebook file.\n",
    "fname = 'scraped_sphinx_code_test.ipynb'\n",
    "# This file opens with no problems the code is excutable \n",
    "# with open(fname, 'w') as f:\n",
    "nbf.writes(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8220\n",
      "8221\n",
      "34\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "print(ord('“'))\n",
    "print(ord('”'))\n",
    "print(ord('\"'))\n",
    "print(ord(\"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create another notebook object.\n",
    "nb = nbf.v4.new_notebook()\n",
    "\n",
    "from ipython_genutils.py3compat import string_types, cast_unicode_py2\n",
    "\n",
    "import tomd\n",
    "\n",
    "from bs4 import UnicodeDammit\n",
    "\n",
    "# tomd.Tomd(chunks[1]).markdown\n",
    "\n",
    "uni = UnicodeDammit(chunks[1], smart_quotes_to=\"ascii\")\n",
    "uni = uni.unicode_markup\n",
    "uni = re.sub('“|”','\"',uni)\n",
    "\n",
    "# Test by formatting a chunk of HTML as a markdown cell.\n",
    "markdown_cell = nbf.v4.new_markdown_cell(uni)\n",
    "nb['cells'].append(markdown_cell)\n",
    "\n",
    "# Write the single cell to a notebook file.\n",
    "# This file fails to open -> Notebook is Unreadable\n",
    "fname = 'scraped_sphinx_markdown_test.ipynb'\n",
    "with open(fname, 'w') as f:\n",
    "    f.write(nbf.writes(nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nbf.writes(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nbf.validate(nb)\n",
    "with open(fname, 'w') as f:\n",
    "    f.write(str(nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': '\\n<p>See <a class=\"reference internal\" href=\"#difflib-interface\"><span class=\"std std-ref\">A command-line interface to difflib</span></a> for a more detailed example.</p>\\n</dd></dl>\\n<dl class=\"function\">\\n<dt id=\"difflib.get_close_matches\">\\n<code class=\"descclassname\">difflib.</code><code class=\"descname\">get_close_matches</code><span class=\"sig-paren\">(</span><em>word</em>, <em>possibilities</em>, <em>n=3</em>, <em>cutoff=0.6</em><span class=\"sig-paren\">)</span><a class=\"headerlink\" href=\"#difflib.get_close_matches\" title=\"Permalink to this definition\">¶</a></dt>\\n<dd><p>Return a list of the best “good enough” matches.  <em>word</em> is a sequence for which\\nclose matches are desired (typically a string), and <em>possibilities</em> is a list of\\nsequences against which to match <em>word</em> (typically a list of strings).</p>\\n<p>Optional argument <em>n</em> (default <code class=\"docutils literal\"><span class=\"pre\">3</span></code>) is the maximum number of close matches to\\nreturn; <em>n</em> must be greater than <code class=\"docutils literal\"><span class=\"pre\">0</span></code>.</p>\\n<p>Optional argument <em>cutoff</em> (default <code class=\"docutils literal\"><span class=\"pre\">0.6</span></code>) is a float in the range [0, 1].\\nPossibilities that don’t score at least that similar to <em>word</em> are ignored.</p>\\n<p>The best (no more than <em>n</em>) matches among the possibilities are returned in a\\nlist, sorted by similarity score, most similar first.</p>\\n'}],\n",
       " 'metadata': {},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cast_unicode_py2(nb, 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipython_genutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipython_genutils.py3compat.cast_unicode('hello', 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cp1252'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipython_genutils.encoding.DEFAULT_ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
